prometheusOperator:
  resources:
    limit:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
prometheus:
  ingress:
    enabled: true
    hosts:
      - prometheus.{{ cluster_zone }}
  prometheusSpec:
    retention: 7d
    scrapeInterval: 30s
alertmanager:
  resources:
    limit:
      cpu: 50m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 128Mi
  config:
    global:
      # The smarthost and SMTP sender used for mail notifications.
      #smtp_smarthost: 'localhost:25'
      #smtp_from: 'alertmanager@example.org'
    
    # The root route on which each incoming alert enters.
    route:
      # The root route must not have any matchers as it is the entry point for
      # all alerts. It needs to have a receiver configured so alerts that do not
      # match any of the sub-routes are sent to someone.
      receiver: slack_operations
    
      # The labels by which incoming alerts are grouped together. For example,
      # multiple alerts coming in for cluster=A and alertname=LatencyHigh would
      # be batched into a single group.
      group_by: ['alertname', 'cluster']
    
      # When a new group of alerts is created by an incoming alert, wait at
      # least 'group_wait' to send the initial notification.
      # This way ensures that you get multiple alerts for the same group that start
      # firing shortly after another are batched together on the first
      # notification.
      group_wait: 15s
    
      # When the first notification was sent, wait 'group_interval' to send a batch
      # of new alerts that started firing for that group.
      group_interval: 5m
    
      # If an alert has successfully been sent, wait 'repeat_interval' to
      # resend them.
      repeat_interval: 3h
    
      # All the above attributes are inherited by all child routes and can
      # overwritten on each.
    
      routes:
      # This routes performs a regular expression match on alert labels to
      # catch alerts that are related to a list of services.

      - match:
          alertname: DeadMansSwitch
          continue: false          

{% if slack_operations_webhook is defined and slack_operations_webhook %}
      - match_re:
          cluster: .*
        receiver: slack_operations
        continue: true
{% endif %}
    
{% if pager_duty_service_key is defined and pager_duty_service_key %}
      - match_re:
          cluster: .*
        receiver: pager_duty
        continue: true
{% endif %}
    
    receivers:
{% if slack_operations_webhook is defined and slack_operations_webhook %}
    - name: slack_operations
      slack_configs:
      - send_resolved: true
        api_url: {{ slack_operations_webhook }}
        channel: {{ slack_operations_channel }}
{% raw %}
        text: "{{ range .Alerts }}{{ range .Annotations.Values }}{{ . }}{{ end }}\nGenerated by {{ .GeneratorURL }}\n{{ end }}"
{% endraw %}
{% endif %}
    
{% if pager_duty_service_key is defined and pager_duty_service_key %}
    - name: pager_duty
      pagerduty_configs:
      - service_key: {{ pager_duty_service_key }}
        send_resolved: true
{% endif %}
    
    # Inhibition rules allow to mute a set of alerts given that another alert is
    # firing.
    # We use this to mute any warning-level notifications if the same alert is
    # already critical.
    inhibit_rules:
    - source_match:
        alertname: DeadMansSwitch
        severity: 'none'
      target_match:
        severity: 'none'

    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      # Apply inhibition if the alertname is the same.
      equal: ['alertname']

grafana:
  ingress:
    enabled: true
    hosts:
      - grafana.{{ cluster_zone }}
kubelet:
  serviceMonitor:
    https: true